
# Git hub repository for code
https://github.com/Niranjan41288/Mining_TwitterData.git

Abstract : 

  As we all know Twitter is a popular social network site where we can post small message called tweets. Users share there thoughts, links and pictures, journalists comment on live events, 
companies promote products and engage with customers. so there’s a lot of data is available to analyse and get some useful information.

 I collected tweets of European football league as league has many matches played so a lot of tweets available.
 
I tried to 

a) make the Predictions about the score on behalf of the fans
b) do some sentiment analysis to find out how team particular team is doing.
c) Information that can’t be retrieved directly from statistics, such as injuries.

So using the twitter we can analyze the tweets during the big leagues or tournaments to keep track of team performance and also make some useful prediction. 






# Import all required libraries

import json # for json data
import pymongo # To insert and fetch the data from mongo database
import tweepy # To fetch the tweets from twitter
import vincent # For plotting the data
import nltk # For natural language processing
from nltk.tokenize import word_tokenize # for tokenize the tweets and to count the term frequency
import pandas # this is most important library to convert into the dataframes for easy accesss 
#from elasticsearch import Elasticsearch

#from flask import Flask
#app = Flask(__name__)

#@app.route("/")
#def main():
#    return "Welcome!"


consumer_key = 'consumer_key '
consumer_secret = 'consumer_secret '

access_key = 'access_key '
access_secret = 'access_secret'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

print('jjjjjjj')
class CustomStreamListener(tweepy.StreamListener):
    def __init__(self, api):
        self.api = api
        super(tweepy.StreamListener, self).__init__()

        self.db = pymongo.MongoClient().test

    def on_data(self, tweet):
        self.db.BPL.insert(json.loads(tweet)) # insert the tweers into the mongodb

    def on_error(self, status_code):
        return True # Don't kill the stream

    def on_timeout(self):
        return True # Don't kill the stream


sapi = tweepy.streaming.Stream(auth, CustomStreamListener(api))
sapi.filter(track=['#BPL'])

# Import all required libraries
import json # for json data
import pymongo # To insert and fetch the data from mongo database
import tweepy # To fetch the tweets from twitter
import vincent # For plotting the data
import nltk
from nltk.tokenize import word_tokenize # for tokenize the tweets and to count the term frequency
import pandas # this is most important library to convert into the dataframes for easy accesss


db = pymongo.MongoClient().test

import re # remove the all hashtags and the common symbols to get the more meaningfull data
 
emoticons_str = r"""
    (?:
        [:=;] # Eyes
        [oO\-]? # Nose (optional)
        [D\)\]\(\]/\\OpP] # Mouth
    )"""
 
regex_str = [
    emoticons_str,
    r'<[^>]+>', # HTML tags
    r'(?:@[\w_]+)', # @-mentions
    r"(?:\#+[\w_]+[\w\'_\-]*[\w_]+)", # hash-tags
    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs
 
    r'(?:(?:\d+,?)+(?:\.?\d+)?)', # numbers
    r"(?:[a-z][a-z'\-_]+[a-z])", # words with - and '
    r'(?:[\w_]+)', # other words
    r'(?:\S)' # anything else
]
    
tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)
 
def tokenize(s):
    return tokens_re.findall(s)
 
def preprocess(s, lowercase=False):
    tokens = tokenize(s)
    if lowercase:
        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]
    return tokens


from nltk.corpus import stopwords
import string

# Generally there are so many stop words in english language. They are very helpful to make sense of the tweets but 
# they are not useful to get any insights from the data so its better that we can get rid of the comman english stop words.

punctuation = list(string.punctuation)
stop = stopwords.words('english') + punctuation + ['rt', 'via']


tweets = db.BPL.find()

terms_stop = []

# terms_stop = [t for t in preprocess(line['text']) for line in tweets if t not in stop]

#terms_stop = [term for line in tweets for term in preprocess(line['text']) if term not in stop]

terms_stop = [term for line in tweets for term in preprocess(line['text']) if term not in stop and 
              not term.startswith(('#', '@'))]
# make the plot

from nltk.book import *
freq = FreqDist(terms_stop)


# Time Series Plot

import vincent

vincent.core.initialize_notebook() # This is for integration of vincent with Ipython notebook

word_freq = freq.most_common(20)
labels, freq = zip(*word_freq)
data = {'data': freq, 'x': labels}
bar = vincent.Bar(data, iter_idx='x')
bar.axis_titles(x='Terms', y='Freq')
bar.display()



tweets = db.BPL.find()

dates_football = []

term_hash = []

        
term_hash = [term for line in tweets for term in preprocess(line['text']) if term.startswith(('#'))]
 
dates_football = [line['created_at'] for line in tweets for term in preprocess(line['text']) if '#Football' in term_hash ]


# a list of "1" to count the hashtags
ones = [1]*len(dates_football)
# the index of the series
idx = pandas.DatetimeIndex(dates_football)
# the actual series (at series of 1s for the moment)
football = pandas.Series(ones, index=idx)
 
# Resampling / bucketing
football = football.resample('1Min', how='sum').fillna(0)


import vincent
vincent.core.initialize_notebook()
Time_chart = vincent.Line(football)
Time_chart.axis_titles(x='Time', y='Freq')
Time_chart.display()


# Do sentiment analysis
# import sentiment_mod as senti
from textblob import TextBlob
import pandas as pd
#from elasticsearch import Elasticsearch


# create instance of elasticsearch
#es = Elasticsearch()

tweets = db.BPL.find()
cursor = db.BPL.find()

tweet_fields = ['text']
tweet_sentiment = ['sentiment']

tweet_list = []

for line in tweets:
    tweet = TextBlob(line['text'])
    #print(tweet.sentiment.polarity)
    if tweet.sentiment.polarity < 0:
        sentiment = "negative"
    elif tweet.sentiment.polarity == 0:
        sentiment = "neutral"
    else:
        sentiment = "positive"
        
    tweet_list.append(sentiment)
    #df1 = concat(df1,df1([{'tweet' : line['text'], 'sentiment' : sentiment }]))

    
#result = pd.DataFrame(tweet, columns=tweet_fields)

#l = tweets['text']
t1 = pd.DataFrame(list(cursor), columns=tweet_fields)
t2 = pd.DataFrame(list(tweet_list), columns=tweet_sentiment)

frames = [t1,t2]

result = pd.concat(frames,axis = 1 )

result[:20]


# Plot the sentiments


%matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib import style
import time


style.use("ggplot")


#fig = plt.figure()
#ax1 = fig.add(subplot(1,1,1))


xar = []
yar = []

x = 0
y = 0

for l in result['sentiment']:
    x +=1
    if "positive" in l:
        y += 1
    elif "negative" in l:
        y -= 1
        
    xar.append(x)
    yar.append(y)


plt.plot(xar,yar)
