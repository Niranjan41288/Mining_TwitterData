
# coding: utf-8

# In[ ]:

# Import all required libraries
import json # for json data
import pymongo # To insert and fetch the data from mongo database
import tweepy # To fetch the tweets from twitter
import vincent # For plotting the data
import nltk
from nltk.tokenize import word_tokenize # for tokenize the tweets and to count the term frequency
import pandas # this is most important library to convert into the dataframes for easy accesss 
#from elasticsearch import Elasticsearch

#from flask import Flask
#app = Flask(__name__)

#@app.route("/")
#def main():
#    return "Welcome!"


consumer_key = 'consumer_key'
consumer_secret = 'consumer_secret'

access_key = 'access_key'
access_secret = 'access_secret'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

class CustomStreamListener(tweepy.StreamListener):
    def __init__(self, api):
        self.api = api
        super(tweepy.StreamListener, self).__init__()

        self.db = pymongo.MongoClient().test

    def on_data(self, tweet):
        self.db.BPL.insert(json.loads(tweet)) # insert the tweers into the mongodb

    def on_error(self, status_code):
        return True # Don't kill the stream

    def on_timeout(self):
        return True # Don't kill the stream


sapi = tweepy.streaming.Stream(auth, CustomStreamListener(api))
sapi.filter(track=['#BPL'])


# In[4]:

# Import all required libraries
import json # for json data
import pymongo # To insert and fetch the data from mongo database
import tweepy # To fetch the tweets from twitter
import vincent # For plotting the data
import nltk
from nltk.tokenize import word_tokenize # for tokenize the tweets and to count the term frequency
import pandas # this is most important library to convert into the dataframes for easy accesss


db = pymongo.MongoClient().test


# In[5]:

import re # remove the all hashtags and the common symbols to get the more meaningfull data
 
emoticons_str = r"""
    (?:
        [:=;] # Eyes
        [oO\-]? # Nose (optional)
        [D\)\]\(\]/\\OpP] # Mouth
    )"""
 
regex_str = [
    emoticons_str,
    r'<[^>]+>', # HTML tags
    r'(?:@[\w_]+)', # @-mentions
    r"(?:\#+[\w_]+[\w\'_\-]*[\w_]+)", # hash-tags
    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs
 
    r'(?:(?:\d+,?)+(?:\.?\d+)?)', # numbers
    r"(?:[a-z][a-z'\-_]+[a-z])", # words with - and '
    r'(?:[\w_]+)', # other words
    r'(?:\S)' # anything else
]
    
tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)
 
def tokenize(s):
    return tokens_re.findall(s)
 
def preprocess(s, lowercase=False):
    tokens = tokenize(s)
    if lowercase:
        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]
    return tokens


# In[6]:

from nltk.corpus import stopwords
import string

# Generally there are so many stop words in english language. They are very helpful to make sense of the tweets but 
# they are not useful to get any insights from the data so its better that we can get rid of the comman english stop words.

punctuation = list(string.punctuation)
stop = stopwords.words('english') + punctuation + ['rt', 'via']


# In[7]:

tweets = db.BPL.find()

terms_stop = []

# terms_stop = [t for t in preprocess(line['text']) for line in tweets if t not in stop]

#terms_stop = [term for line in tweets for term in preprocess(line['text']) if term not in stop]

terms_stop = [term for line in tweets for term in preprocess(line['text']) if term not in stop and 
              not term.startswith(('#', '@'))]


# Concept:(http://vincent.readthedocs.org/en/latest/)
# 
# The data capabilities of Python. The visualization capabilities of JavaScript.
# Vincent allows you to build Vega specifications in a Pythonic way, and performs type-checking to help ensure that your specifications are correct. It also has a number of convenience chart-building methods that quickly turn Python data structures into Vega visualization grammar, enabling graphical exploration. It allows for quick iteration of visualization designs via getters and setters on grammar elements, and outputs the final visualization to JSON.
# 
# Perhaps most importantly, Vincent has Pandas-Fu, and is built specifically to allow for quick plotting of DataFrames and Series.

# In[8]:

from nltk.book import *
freq = FreqDist(terms_stop)

import vincent

vincent.core.initialize_notebook() # This is for integration of vincent with Ipython notebook

word_freq = freq.most_common(20)
labels, freq = zip(*word_freq)
data = {'data': freq, 'x': labels}
bar = vincent.Bar(data, iter_idx='x')
bar.axis_titles(x='Terms', y='Freq')
bar.display()


# In[9]:

#import pandas 

tweets = db.BPL.find()

dates_football = []

term_hash = []

        
term_hash = [term for line in tweets for term in preprocess(line['text']) if term.startswith(('#'))]
 
dates_football = [line['created_at'] for line in tweets for term in preprocess(line['text']) if '#Football' in term_hash ]


# a list of "1" to count the hashtags
ones = [1]*len(dates_football)
# the index of the series
idx = pandas.DatetimeIndex(dates_football)
# the actual series (at series of 1s for the moment)
football = pandas.Series(ones, index=idx)
 
# Resampling / bucketing
football = football.resample('1Min', how='sum').fillna(0)


# In[10]:

import vincent
vincent.core.initialize_notebook()
Time_chart = vincent.Line(football)
Time_chart.axis_titles(x='Time', y='Freq')
Time_chart.display()


# In[11]:

# import sentiment_mod as senti
from textblob import TextBlob
import pandas as pd
#from elasticsearch import Elasticsearch


# create instance of elasticsearch
#es = Elasticsearch()

tweets = db.BPL.find()
cursor = db.BPL.find()

tweet_fields = ['text']
tweet_sentiment = ['sentiment']

tweet_list = []

for line in tweets:
    tweet = TextBlob(line['text'])
    #print(tweet.sentiment.polarity)
    if tweet.sentiment.polarity < 0:
        sentiment = "negative"
    elif tweet.sentiment.polarity == 0:
        sentiment = "neutral"
    else:
        sentiment = "positive"
        
    tweet_list.append(sentiment)
    #df1 = concat(df1,df1([{'tweet' : line['text'], 'sentiment' : sentiment }]))

    
#result = pd.DataFrame(tweet, columns=tweet_fields)

#l = tweets['text']
t1 = pd.DataFrame(list(cursor), columns=tweet_fields)
t2 = pd.DataFrame(list(tweet_list), columns=tweet_sentiment)

frames = [t1,t2]

result = pd.concat(frames,axis = 1 )

result[:20]


# In[12]:

# Plot the sentiments


get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib import style
import time


style.use("ggplot")


#fig = plt.figure()
#ax1 = fig.add(subplot(1,1,1))


xar = []
yar = []

x = 0
y = 0

for l in result['sentiment']:
    x +=1
    if "positive" in l:
        y += 1
    elif "negative" in l:
        y -= 1
        
    xar.append(x)
    yar.append(y)


plt.plot(xar,yar)

